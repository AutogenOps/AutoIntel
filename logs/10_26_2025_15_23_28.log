{"timestamp": "2025-10-26T09:53:40.059386Z", "level": "info", "event": "Initializing ApiKeyManager"}
{"timestamp": "2025-10-26T09:53:40.059721Z", "level": "info", "event": "OPENAI_API_KEY loaded successfully from environment"}
{"timestamp": "2025-10-26T09:53:40.059960Z", "level": "info", "event": "GOOGLE_API_KEY loaded successfully from environment"}
{"timestamp": "2025-10-26T09:53:40.060186Z", "level": "info", "event": "GROQ_API_KEY loaded successfully from environment"}
{"path": "C:\\Users\\Akash\\LLMOPS\\LLMOPS\\automated-research-report-generation-final\\research_and_analyst\\config\\configuration.yaml", "keys": ["astra_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-10-26T09:53:40.062483Z", "level": "info", "event": "Configuration loaded successfully"}
{"config_keys": ["astra_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-10-26T09:53:40.079935Z", "level": "info", "event": "YAML configuration loaded successfully"}
{"provider": "openai", "model": "gpt-4o", "timestamp": "2025-10-26T09:53:40.080299Z", "level": "info", "event": "Loading LLM"}
{"provider": "openai", "model": "gpt-4o", "timestamp": "2025-10-26T09:53:41.107929Z", "level": "info", "event": "LLM loaded successfully"}
{"module": "AutonomousReportGenerator", "timestamp": "2025-10-26T09:53:41.109167Z", "level": "info", "event": "Building report generation graph"}
{"module": "InterviewGraphBuilder", "timestamp": "2025-10-26T09:53:41.109843Z", "level": "info", "event": "Building Interview Graph workflow"}
{"module": "InterviewGraphBuilder", "timestamp": "2025-10-26T09:53:41.118960Z", "level": "info", "event": "Interview Graph compiled successfully"}
{"module": "AutonomousReportGenerator", "timestamp": "2025-10-26T09:53:41.127572Z", "level": "info", "event": "Report generation graph built successfully"}
{"module": "ReportService", "topic": "effect of genai on developers", "thread_id": "2f4160a4-e544-44dd-885d-05c441bb4b6b", "timestamp": "2025-10-26T09:53:41.127910Z", "level": "info", "event": "Starting report pipeline"}
{"module": "AutonomousReportGenerator", "topic": "effect of genai on developers", "timestamp": "2025-10-26T09:53:41.130086Z", "level": "info", "event": "Creating analyst personas"}
HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
Retrying request to /chat/completions in 0.472941 seconds
HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
Retrying request to /chat/completions in 0.997126 seconds
HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
{"module": "AutonomousReportGenerator", "error": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}", "timestamp": "2025-10-26T09:53:44.639625Z", "level": "error", "event": "Error creating analysts"}
{"module": "ReportService", "error": "Error in [C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py] at line [1047] | Message: Failed to create analysts\nTraceback:\nTraceback (most recent call last):\n  File \"C:\\Users\\Akash\\LLMOPS\\LLMOPS\\automated-research-report-generation-final\\research_and_analyst\\workflows\\report_generator_workflow.py\", line 65, in create_analyst\n    analysts = structured_llm.invoke([\n        SystemMessage(content=system_prompt),\n        HumanMessage(content=\"Generate the set of analysts.\"),\n    ])\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3244, in invoke\n    input_ = context.run(step.invoke, input_, config, **kwargs)\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5711, in invoke\n    return self.bound.invoke(\n           ~~~~~~~~~~~~~~~~~^\n        input,\n        ^^^^^^\n        self._merge_configs(config),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **{**self.kwargs, **kwargs},\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n    ~~~~~~~~~~~~~~~~~~~~^\n        [self._convert_input(input)],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ).generations[0][0],\n    ^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1025, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 842, in generate\n    self._generate_with_cache(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n        m,\n        ^^\n    ...<2 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1091, in _generate_with_cache\n    result = self._generate(\n        messages, stop=stop, run_manager=run_manager, **kwargs\n    )\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1183, in _generate\n    raise e\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1151, in _generate\n    self.root_client.chat.completions.with_raw_response.parse(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        **payload\n        ^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 183, in parse\n    return self._post(\n           ~~~~~~~~~~^\n        \"/chat/completions\",\n        ^^^^^^^^^^^^^^^^^^^^\n    ...<49 lines>...\n        stream=False,\n        ^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n", "timestamp": "2025-10-26T09:53:44.646706Z", "level": "error", "event": "Error initiating report generation"}
